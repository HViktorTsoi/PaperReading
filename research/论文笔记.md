# PointNet
http://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf

# PointNet++
http://papers.nips.cc/paper/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space.pdf
MSG：取不同半径的点的特征拼接在一起
MRG：高层少数点特征pool+低层对应这些少数点的多数点特征pool拼接
无序主要影响的是没有局部信息

# SehllNet
http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_ShellNet_Efficient_Point_Cloud_Convolutional_Neural_Networks_Using_Concentric_Shells_ICCV_2019_paper.pdf

# Octree guided CNN with Spherical Kernels for 3D Point Clouds
https://arxiv.org/pdf/1903.00343.pdf

# PointCNN
邻居点feature和邻居点xyz提取的feature拼接，由于不同邻居点的顺序对生成不同的点集，学习一个排列矩阵，将邻居点顺序进行重新排列，从而实现点顺序的无关性质

# Deep Snake
Deep Snake Head：
1 输入是轮廓的N个点
2 经过多次circle conv，生成轮廓feature
3 进行1次1x1 conv，再max pooling，生成整个轮廓的全局feature(这里为什么用max pooling？汇集全局信息应该用avg pool啊)
4 再进行多次1x1 conv，得到最终的每个轮廓点的offset
用轮廓作为分割边界
1 首先生成detection的bb
2 选择bb四个中点，生成菱形
3 过一次Deep snake 生成物体的四个角点的offset(为了丰富边缘特征，实际上输入的40个点对应的feature，每个边被插值成了10个点，但是最后loss只负责监督输出的角点到bb的四角的offset)
4 分别以四个角点为中点，扩展出四条水平/垂直线(这四条线实际上就位于bb的四条边上)，其长度是bb的1/4；然后再把这四条线的两边连起来，最终构成一个八边形。这个八边形就比原来的矩形bb更好的框住了物体的轮廓，就把这个八边形作为snake的初始轮廓。
5 将这个初始轮廓采样成N个点，再送进Deep Snake，得到最终轮廓的N个offset(N一般是128)
6 重复i次步骤5(生成offset后，对原来的轮廓进行deform，然后将新轮廓采样之后再送入deep snake)，因为有的点离GT过远，需要反复进行偏移才能到GT附近
7 对于被遮挡物割裂的物体，使用multi component detector，先对Feature map做roi align，然后用一个多组件detector来检测目标，分别对每个组件做snake，最后再将轮廓合并

# Adder Net
把原来求互相关的conv转化为求L1-Norm(差的绝对值)

在bp过程中，原本的导数是sgn(X-F)，但是为了训练稳定，改为X-F，更精确；但是这个值可能太大(大于原本的+-1之间)，因此对与往前一层传的导数还要进行clip到+-1之间，使优化更稳定。

由于Adder conv的输出方差更大(见原论文)，需要借助BN来处理其输出；但是由BN的反传公式，方差越大，在BN层往前传的梯度越小，学不动了，因此需要适当增大学习率；但是通过实验发现，不同层的梯度大小分布也不同，因此不能对所有层的学习率都增大相同的倍数，而是使用了a~l~这个可调整系数，它与对应参数梯度的l2-norm成反比，也就是梯度越小，学习率相对更高，这样保证网络的不同层参数在优化时的实际更新的梯度是接近的。

# YOLO v4
## Bag of freebies
### 数据增强
随机裁剪旋转；
Random erase，CutOut；
DropOut；
MixUp，CutMix；

### 数据分布
hard example mining;
focal loss;
label smoothing;

### BBox regression
普通的BBox regression(回归角点，回归到anchor的offset)；
IOU Loss，GIOU Loss，DIOU Loss，CIOU Loss；

##  Bag of specials
### 增加感受野
SPP，ASPP，RFB；

### Attention
SEBlock,SAM;

### Feature integration
FPN,SFAM,ASFF,BiFPN;

### Activation
LReLU,PReLU,ReLU6,Swish,Mish;

### Post processing
Soft NMS；

## Backbone
需要感受野大，并且网络参数大，且输入图像够大
CSPDarknet53 + SPP + PANet + YOLOv3 Head

## 数据增强
Mosaic：就是大小为4的CutMix，找四张图像分别随机裁剪一小块下来，然后拼接在一起。这样可以让网络在被feed进一张图像时，接触到更多种类的contex的信息；另外就是BN在计算分布的时候用到了4张图像的统计信息，就不需要更大的batch size。

SAT：Self-Adversarial Training，循环执行两轮，第一轮固定网络参数训练扰动，目标是使检测结果达到最差；第二轮固定扰动正常训练网络参数，目的是在有扰动的情况下让检测结果达到最好
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/04/24/1587724175754-1587724175834.png)

CmBN：modified CBN，将1个batch划分为4个mini batch，对每一个mini Batch，分别accumulate W，再accumulate 这个mini batch对应的BN(传统的BN是accumulate这个大的batch中所有已计算的mini batchBN)，然后normalize BN，但是不更新W和BN的参数，直到最后一个mini batch算完之后再更新参数。

modified SAM：Spatial Attention Module
modified PAN：把add换成了concat


# Grid-GCN for Fast and Scalable Point Cloud Learning
Liu et al. [26] show that the data structuring cost in three popular point-based models [22, 45, 40] is up to 88% of the overall computational cost. 

提出了Grid-GCN，Coverage-Aware Grid Query (CAGQ).
1. 首先voxelize；
2. $O_v$是所有非空voxel，从中采样M个中心voxel $O_c$；
3. 因为已经构建了point-voxel的对应关系，就可以直接从映射表中查询某个中心voxel的邻居voxel（称之为context points）；
4. 从contex points中