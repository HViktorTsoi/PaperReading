Faster RCNN中RPN的Anchors原理解析：

得到60*40的feature map之后，对于map上的每一个像素，分别由

- 1x1x18(9 * 2)卷积生成每个像素的9个Anchor的分类信息，即该Anchor是否有目标

- 1x1x36(9 * 4)卷积生成每个像素的9个Anchot的偏移信息(Dx,Dy,Dw,Dh)，这个信息是用来加到固定的Anchor上的，也就是说，在配置中，我们固定了Anchor有3个大小，3中尺度，共3*3=9个Anchor，但是这种固定大小的肯定是和GroundTruth的大小不能完全匹配，而且负责生成这个Anchor的像素的位置和GroundTruth的中心位置也不能匹配，因此这第二个1x1卷积就是用来尽量的学到固定大小的Anchor和GT之间的大小的位置差距，从而使Proposal更加准确(当然没有Stage 2 的回归更准确，这里只是说相对的准确)。

其中，固定的基准Anchor是在训练一开始，初始化模型的时候就一次性生成好了，而不是每个像素进行1x1卷积时才生成的，这个固定的基准Anchor+RPN预测的偏移值，就得到了Proposal。

下边这段存疑,RPN生成的proposal应该就是anchor到GT Box之间的偏移,在求RPN_CLS_LOSS和RPN_PRED_LOSS时求的是anchor+proposal偏移之后和GT Box之间的差距.
> 在给每个像素生成固定Anchor之后，且在固定Anchor加上1x1conv的偏移之前，就需要求RPN_CLS_LOSS和RPN_PRED_LOSS，通过ground truth box与固定的基准anchor之间的大小和位置差异来进行学习，从而使RPN网络中的权重能够学习到预测box的能力。(这里没有求GT和"已经加了偏移的Anchor"之间的差距，从而进行学习的原因是，因为网络在每一轮训练或者预测开始时，只能获得到固定基准Anchor的大小和位置，无法获得上一时刻已经加了未经修正的偏移的、更准确的Anchor位置，因此实际上RPN要学的是基准Anchor和GT之间的差距，让这个差距越来越接近真实的差距，而不是学怎么预测出一个框使得这个框和GT特别接近，如果是这样的话Anchor就没有意义了)。

因此这里Anchor的作用就是，给RPN提供一个回归的基准，如果Anchor本身就很接近大部分目标的形状了，那么RPN就只需要很少的学习量，能够很好的提升学习性能；如果只用1个Anchor，或者不用Anchor，或者基础Anchor的大小和GT相差很大，那么RPN就需要很大的功夫去学习生成的Proposal和GT之间的偏差，甚至可能不收敛。
